{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "imdb_sentiment_saved.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXJrR4TKxMnl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
        "from keras.datasets import imdb\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from keras import backend as K\n",
        "K.tensorflow_backend._get_available_gpus()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IY5yVn2LxVuQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "max_features = 20000\n",
        "# cut texts after this number of words\n",
        "# (among top max_features most common words)\n",
        "maxlen = 100\n",
        "batch_size = 32\n",
        "index_from=3\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAeRjL3By3wh",
        "colab_type": "text"
      },
      "source": [
        "## Загрузим данные\n",
        "Нам нужны только 20к самых популярных слов\n",
        "\n",
        "Остальное забьем токеном UNK, он же OOV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_b_xcPhxYVJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features, index_from=3)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECVRmoZPzjRV",
        "colab_type": "text"
      },
      "source": [
        "Вот так выглядят закодированные отзывы"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxDIWYBEzPeu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(x_train[5]) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJ7vrYgTzrgt",
        "colab_type": "text"
      },
      "source": [
        "## Паддинг\n",
        "В целях эффективности(выделение памяти под тензоры, работа с ними) рекуррентные сетки принято кормить батчами последовательностей одной длины.\n",
        "\n",
        "Можно, конечно, попробовать побить их на батчи примерно одной длины, чтобы паддить меньше, но мы будем решать задачу в лоб:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYWQ49Ldxavb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Pad sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3afKT8K0cU2",
        "colab_type": "text"
      },
      "source": [
        "## Построение индекса слов\n",
        "\n",
        "Эта штука нам пригодится в продакшне, а так же для отладки.\n",
        "\n",
        "Keras резерзвирует 0 как специальный символ для паддинга в своих Embedding-слоях.\n",
        "\n",
        "\n",
        "*   В датасете IMDB 1 занята специальным токеном \"начало предложения\"\n",
        "*   2 занята токеном UNK/OOV\n",
        "*   3 не используется в виду ошибки в коде(https://jamesmccaffrey.wordpress.com/2018/04/27/inspecting-the-imdb-dataset-reverse-mapping-the-index-values/)\n",
        "\n",
        "\n",
        "учтем это при построении индекса: сместим все индексы на 3\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3wg8QqaxB6l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_id = imdb.get_word_index()\n",
        "word_to_id = {k:(v+index_from) for k,v in word_to_id.items()}\n",
        "word_to_id[\"<PAD>\"] = 0\n",
        "word_to_id[\"<START>\"] = 1\n",
        "word_to_id[\"<UNK>\"] = 2\n",
        "word_to_id[\"<UNUSED>\"] = 3\n",
        "\n",
        "id_to_word = {value:key for key,value in word_to_id.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOSFm9781uL_",
        "colab_type": "text"
      },
      "source": [
        "## Конструируем модельку\n",
        "\n",
        "\n",
        "\n",
        "1.   Эмбеддинги по количеству токенов(20к + 4 специальных)\n",
        "2.   Двунаправленный LSTM\n",
        "3.   Дропаут(она все равно переобучится в хлам, но мы \"пока об этом не знаем\")\n",
        "4.   Логистическая регрессия, классифицирующая стейт LSTMки\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Piy_Th5fxd3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(max_features + index_from + 1, 128, input_length=maxlen))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiKZEfqyxgG6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# try using different optimizers and different optimizer configs\n",
        "from keras.callbacks import History\n",
        "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YG1KZAs42hO2",
        "colab_type": "text"
      },
      "source": [
        "## Оверфитнем все\n",
        "No comments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsGmZ8tTxhgq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Train...')\n",
        "history = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=4,\n",
        "          validation_data=[x_test, y_test])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMcBTgcSxiHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWWRCBzPGhr6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(' '.join(id_to_word[id] for id in x_train[4])) ## Проверка, что wordindex построен правильно"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiS2y0V320za",
        "colab_type": "text"
      },
      "source": [
        "## Выгружаем модель и индекс"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HL-qL1gg7qCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('clf.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCgoQmhy-0NU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "with open('word_index.json', 'w') as f:\n",
        "  f.write(json.dumps(word_to_id))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC9U4Sp7GLMs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "keras.backend.floatx() ## Лучше бы знать, сколькибитный float использовал keras на обучении. Потом можно наловить с этим проблем"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fUsQaY03Ip8",
        "colab_type": "text"
      },
      "source": [
        "# Идеи для улучшешия\n",
        "\n",
        "\n",
        "*   Использовать взрослый препроцессинг(лемматизация/стемминг)\n",
        "*   Использовать предобученные word embeddings(fasttext, glove, word2vec)  (упороться и предобучить самостоятельно?)\n",
        "*   Доучить предобученные эмбеддинги\n",
        "*   Доучить предобученные эмбеддинги, не испортив их\n",
        "*   Более разумно формировать словарь(подсказка: посмотрите на первое слово в индексе после специальных токенов. Они отсортированы по популярности)\n",
        "*   Побороть переобучение(обратите внимание на train/test split и ширину LSTM)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uynssOUV6C99",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}